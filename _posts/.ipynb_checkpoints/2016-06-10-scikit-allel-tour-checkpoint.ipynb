{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tour of scikit-allel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "This notebook contains a brief tour of some exploratory analyses that can be performed on large-scale genome variation data using [scikit-allel](http://scikit-allel.readthedocs.org) in combination with general purpose Python scientific libraries such as [numpy](http://numpy.org), [pandas](http://pandas.pydata.org), [matplotlib](http://matplotlib.org), etc.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "In the [Ag1000G](http://www.malariagen.net/ag1000g) project (phase 1) we have whole-genome Illumina sequence data from 765 individual mosquitoes collected from 8 different countries across Africa. We have used a [standard approach](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3083463/?tool=pmcentrez) to find putative genetic differences between each individual mosquito and the reference genome sequence. The raw (i.e., unfiltered) data on genetic variantion contains more than 90 million single nucleotide polymorphisms (SNPs).\n",
    "\n",
    "There are a range of analyses we'd like to perform with these data. The raw data are noisy and so we need to identify poor quality features within the data and design filters. After we've dealt with data quality, we would like to learn about the different populations that these mosquitoes came from. For example, are these populations genetically different from each other, and have any genes have come under recent selection (e.g., due to insecticides).\n",
    "\n",
    "These are rich data and there are many ways they could be explored. The goal of [scikit-allel]() is to help speed up this exploration process by providing a toolkit for data manipulation and descriptive statistics. In particular, we would like to enable interactive analysis of large scale data on commodity hardware. This means that functions should be able to run on ~100 million genetic variants in ~1000 individuals on a moderate spec desktop or laptop computer and return in seconds or at most minutes. I.e., you should be able to analyse data wherever you are (at your desk, on a train, ...) and only the occasional tea break should be required.\n",
    "\n",
    "### About this tour\n",
    "\n",
    "The goal of this tour is to give a flavour of:\n",
    "\n",
    "* What types of analysis are possible\n",
    "* How much coding is required\n",
    "* How fast stuff runs\n",
    "\n",
    "For more information see the Further Reading section below. The raw .ipynb file for this post is available [here](https://github.com/alimanfoo/alimanfoo.github.io/tree/master/_posts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the libraries we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_style('ticks')\n",
    "sns.set_context('talk')\n",
    "import h5py\n",
    "import allel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll be analysing originally came from a [VCF format file](https://en.wikipedia.org/wiki/Variant_Call_Format), however these data have previously been pre-processed into an [HDF5 file](https://www.hdfgroup.org/HDF5/) which improves data access speed for a range of access patterns. I won't cover this pre-processing step here, for more information see [vcfnp](https://github.com/alimanfoo/vcfnp). \n",
    "\n",
    "Open an HDF5 file containing variant calls from the [Ag1000G project phase 1 AR3 data release](https://www.malariagen.net/data/ag1000g-phase1-ar3). Note that this contains **raw data**, i.e., all putative SNPs are included. As part of the Ag1000G project we have done a detailed analysis of data quality and so this dataset already contains filter annotations to tell you which SNPs we believe are real. However, for the purpose of this tour, I am going to pretend that we have never seen these data before, and so need to perform our own analysis of data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "callset_fn = '/data/coluzzi/ag1000g/data/phase1/release/AR3/variation/main/hdf5/ag1000g.phase1.ar3.h5'\n",
    "callset = h5py.File(callset_fn, mode='r')\n",
    "callset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a chromosome arm to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chrom = '3L'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize variant density\n",
    "\n",
    "As a first step into getting to know these data, let's make a plot of variant density, which will simply show us how many SNPs there are and how they are distributed along the chromosome.\n",
    "\n",
    "The data on SNP positions and various other attributes of the SNPs are stored in the HDF5 file. Each of these can be treated as a column in a table, so let's set up a table with some of the columns we'll need for this and subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variants = allel.VariantChunkedTable(callset[chrom]['variants'], \n",
    "                                     names=['POS', 'REF', 'ALT', 'DP', 'MQ', 'QD', 'num_alleles'],\n",
    "                                     index='POS')\n",
    "variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The caption for this table tells us that we have 16,437,135 SNPs (rows) in this dataset.\n",
    "\n",
    "Now let's extract the variant positions and load into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = variants['POS'][:]\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to plot variant density in windows over the chromosome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_windowed_variant_density(pos, window_size, title=None):\n",
    "    \n",
    "    # setup windows \n",
    "    bins = np.arange(0, pos.max(), window_size)\n",
    "    \n",
    "    # use window midpoints as x coordinate\n",
    "    x = (bins[1:] + bins[:-1])/2\n",
    "    \n",
    "    # compute variant density in each window\n",
    "    h, _ = np.histogram(pos, bins=bins)\n",
    "    y = h / window_size\n",
    "    \n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    sns.despine(ax=ax, offset=10)\n",
    "    ax.plot(x, y)\n",
    "    ax.set_xlabel('Chromosome position (bp)')\n",
    "    ax.set_ylabel('Variant density (bp$^{-1}$)')\n",
    "    if title:\n",
    "        ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a plot with the SNP positions from our chosen chromosome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_windowed_variant_density(pos, window_size=100000, title='Raw variant density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that variant density is around 0.5 over much of the chromosome, which means the raw data contains a SNP about every 2 bases of the reference genome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore variant attributes\n",
    "\n",
    "As I mentioned above, each SNP also has a number \"annotations\", which are data attributes that originally came from the \"INFO\" field in the VCF file. These are important for data quality, so let's begin by getting to know a bit more about the numerical range and distribution of some of these attributes.\n",
    "\n",
    "Each attribute can be loaded from the table we setup earlier into a numpy array. E.g., load the \"DP\" field into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variants['DP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp = variants['DP'][:]\n",
    "dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to plot a frequency distribution for any variant attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_variant_hist(f, bins=30):\n",
    "    x = variants[f][:]\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    sns.despine(ax=ax, offset=10)\n",
    "    ax.hist(x, bins=bins)\n",
    "    ax.set_xlabel(f)\n",
    "    ax.set_ylabel('No. variants')\n",
    "    ax.set_title('Variant %s distribution' % f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"DP\" is total depth of coverage across all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_variant_hist('DP', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"MQ\" is average mapping quality across all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_variant_hist('MQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"QD\" is a slightly odd statistic but turns out to be very useful for finding poor quality SNPs. Roughly speaking, high numbers mean that evidence for variation is strong (concentrated), low numbers mean that evidence is weak (dilute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_variant_hist('QD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's see how many biallelic, triallelic and quadriallelic SNPs we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_variant_hist('num_alleles', bins=np.arange(1.5, 5.5, 1))\n",
    "plt.gca().set_xticks([2, 3, 4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the joint frequency distribution of two attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_variant_hist_2d(f1, f2, downsample):\n",
    "    x = variants[f1][:][::downsample]\n",
    "    y = variants[f2][:][::downsample]\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    sns.despine(ax=ax, offset=10)\n",
    "    ax.hexbin(x, y, gridsize=40)\n",
    "    ax.set_xlabel(f1)\n",
    "    ax.set_ylabel(f2)\n",
    "    ax.set_title('Variant %s versus %s joint distribution' % (f1, f2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the plotting go faster I've downsampled to use every 10th variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_variant_hist_2d('QD', 'MQ', downsample=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate variant quality\n",
    "\n",
    "The DP, MQ and QD attributes are potentially informative about SNP quality. For example, we have a prior expectation that putative SNPs with very high or very low DP may coincide with some form of larger structural variation, and may therefore be unreliable. However, it would be great to have some empirical indicator of data quality, which could guide our choices about how to filter the data.\n",
    "\n",
    "There are several possible quality indicators that could be used, and in general it's a good idea to use more than one if available. Here, to illustrate the general idea, let's use just one indicator, which is the number of [transitions]() divided by the number of [transversions](), which I will call Ti/Tv.\n",
    "\n",
    "![Transitions and transversions](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Transitions-transversions-v4.svg/500px-Transitions-transversions-v4.svg.png)\n",
    "\n",
    "If mutations were completely random we would expect a Ti/Tv of 0.5, because there are twice as many possible transversions as transitions. However, in most species a mutation bias has been found towards transitions, and so we expect the true Ti/Tv to be higher. We can therefore look for features of the raw data that are associated with low Ti/Tv (close to 0.5) and be fairly confident that these contain a lot of noise. \n",
    "\n",
    "To do this, let's first set up an array of mutations, where each entry contains two characters representing the reference and alternate allele. For simplicity of presentation I'm going to ignore the fact that some SNPs are multiallelic, but if doing this for real this should be restricted to biallelic variants only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mutations = np.char.add(variants['REF'], variants['ALT'][:, 0])\n",
    "mutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to locate transition mutations within a mutations array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def locate_transitions(x):\n",
    "    x = np.asarray(x)\n",
    "    return (x == b'AG') | (x == b'GA') | (x == b'CT') | (x == b'TC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate how the ``locate_transitions`` function generates a boolean array from a mutations array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_ti = locate_transitions(mutations)\n",
    "is_ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to compute Ti/Tv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ti_tv(x):\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    is_ti = locate_transitions(x)\n",
    "    n_ti = np.count_nonzero(is_ti)\n",
    "    n_tv = np.count_nonzero(~is_ti)\n",
    "    if n_tv > 0:\n",
    "        return n_ti / n_tv\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate the ``ti_tv`` function by computing Ti/Tv over all SNPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ti_tv(mutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to plot Ti/Tv in relation to a variant attribute like DP or MQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_ti_tv(f, downsample, bins):\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    sns.despine(ax=ax, offset=10)\n",
    "    x = variants[f][:][::downsample]\n",
    "    \n",
    "    # plot a histogram\n",
    "    ax.hist(x, bins=bins)\n",
    "    ax.set_xlabel(f)\n",
    "    ax.set_ylabel('No. variants')\n",
    "\n",
    "    # plot Ti/Tv\n",
    "    ax = ax.twinx()\n",
    "    sns.despine(ax=ax, bottom=True, left=True, right=False, offset=10)\n",
    "    values = mutations[::downsample]\n",
    "    y1, _, _ = scipy.stats.binned_statistic(x, values, statistic=ti_tv, bins=bins)\n",
    "    bx = (bins[1:] + bins[:-1]) / 2\n",
    "    ax.plot(bx, y1, color='k')\n",
    "    ax.set_ylabel('Ti/Tv')\n",
    "    ax.set_ylim(0.6, 1.3)\n",
    "\n",
    "    ax.set_title('Variant %s and Ti/Tv' % f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example the relationship between the QD, MQ and DP attributes and Ti/Tv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ti_tv('QD', downsample=5, bins=np.arange(0, 40, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ti_tv('MQ', downsample=5, bins=np.arange(0, 60, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ti_tv('DP', downsample=5, bins=np.linspace(0, 50000, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ti/Tv is not a simple variable and so some care is required when interpreting these plots. However, we can see that there is a trend towards low Ti/Tv for low values of QD, MQ and DP.\n",
    "\n",
    "To investigate further, let's look at Ti/Tv in two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_joint_ti_tv(f1, f2, downsample, gridsize=20, mincnt=20, vmin=0.6, vmax=1.4, extent=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.despine(ax=ax, offset=10)\n",
    "    x = variants[f1][:][::downsample]\n",
    "    y = variants[f2][:][::downsample]\n",
    "    C = mutations[::downsample]\n",
    "    im = ax.hexbin(x, y, C=C, reduce_C_function=ti_tv, mincnt=mincnt, extent=extent,\n",
    "                   gridsize=gridsize, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    fig.colorbar(im)\n",
    "    ax.set_xlabel(f1)\n",
    "    ax.set_ylabel(f2)\n",
    "    ax.set_title('Variant %s versus %s and Ti/Tv' % (f1, f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_joint_ti_tv('QD', 'MQ', downsample=5, mincnt=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_joint_ti_tv('QD', 'DP', downsample=5, mincnt=400, extent=(0, 40, 0, 50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_joint_ti_tv('MQ', 'DP', downsample=5, mincnt=400, extent=(0, 60, 0, 50000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information may be useful when designing a variant filtering strategy. If you have other data that could be used as a quality indicator, such as Mendelian errors in a trio or cross, and/or data on genotype discordances between replicate samples, a similar analysis could be performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Filtering variants\n",
    "\n",
    "There are many possible approaches to filtering variants. The simplest approach is define thresholds on variant attributes like DP, MQ and QD, and exclude SNPs that fall outside of a defined range (a.k.a. \"hard filtering\"). This is crude but simple to implement and in many cases may suffice, at least for an initial exploration of the data. \n",
    "\n",
    "Let's implement a simple hard filter. First, a reminder that we have a table containing all these variant attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hard filter using an expression. This is just a string of Python code, which we will evaluate in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_expression = '(QD > 5) & (MQ > 40) & (DP > 15000) & (DP < 30000)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the filter using the columns from the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variant_selection = variants.eval(filter_expression)[:]\n",
    "variant_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many variants to we keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(variant_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many variants do we filter out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(~variant_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our variant filter, let's make a new variants table with only rows for variants that pass our filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variants_pass = variants.compress(variant_selection)\n",
    "variants_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset genotypes\n",
    "\n",
    "Now that we have some idea of variant quality, let's look at our samples and at the genotype calls.\n",
    "\n",
    "All data relating to the genotype calls is stored in the HDF5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calldata = callset[chrom]['calldata']\n",
    "calldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(calldata.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these is a separate dataset in the HDF5 file. To make it easier to work with the genotype dataset, let's wrap it using a class from scikit-allel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genotypes = allel.GenotypeChunkedArray(calldata['genotype'])\n",
    "genotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B., at this point we have not loaded any data into memory, it is still in the HDF5 file. From the representation above we have some diagnostic information about the genotypes, for example, we have calls for 16,437,135 variants in 765 samples with ploidy 2 (i.e., diploid). Uncompressed these data would be 23.4G but the data are compressed and so actually use 1.2G on disk.\n",
    "\n",
    "We can also see genotype calls for the first 5 variants in the first and last 5 samples, which are all missing (\"./.\").\n",
    "\n",
    "Before we go any furter, let's also pull in some data about the 765 samples we've genotyped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples_fn = '/data/coluzzi/ag1000g/data/phase1/release/AR3/samples/samples.meta.txt'\n",
    "samples = pandas.DataFrame.from_csv(samples_fn, sep='\\t')\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"population\" column defines which of 9 populations the mosquitoes came from, based on sampling location and species. E.g., \"BFM\" means *Anopheles coluzzii* mosquitoes from Burkina Faso, \"GAS\" means *Anopheles gambiae* mosquitoes from Gabon. How many mosquitoes come from each of these populations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples.population.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with two populations only for simplicity. These are *Anopheles coluzzii* populations from Burkina Faso (BFM) and Angola (AOM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_selection = samples.population.isin({'BFM', 'AOM'}).values\n",
    "sample_selection[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now restrict the samples table to only these two populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples_subset = samples[sample_selection]\n",
    "samples_subset.reset_index(drop=True, inplace=True)\n",
    "samples_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples_subset.population.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's subset the genotype calls to keep only variants that pass our quality filters and only samples in our two populations of interest. This takes a couple of minutes, so time for a quick tea break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "genotypes_subset = genotypes.subset(variant_selection, sample_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genotypes_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new genotype array we've made has 11,766,616 SNPs and 129 samples, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample QC\n",
    "\n",
    "Before we go any further, let's do some sample QC. This is just to check if any of the 129 samples we're working with have major quality issues that might confound an analysis. \n",
    "\n",
    "Compute the percent of missing and heterozygous genotype calls for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_variants = len(variants_pass)\n",
    "pc_missing = genotypes_subset.count_missing(axis=0)[:] * 100 / n_variants\n",
    "pc_het = genotypes_subset.count_het(axis=0)[:] * 100 / n_variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to plot genotype frequencies for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_genotype_frequency(pc, title):\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    sns.despine(ax=ax, offset=10)\n",
    "    left = np.arange(len(pc))\n",
    "    palette = sns.color_palette()\n",
    "    pop2color = {'BFM': palette[0], 'AOM': palette[1]}\n",
    "    colors = [pop2color[p] for p in samples_subset.population]\n",
    "    ax.bar(left, pc, color=colors)\n",
    "    ax.set_xlim(0, len(pc))\n",
    "    ax.set_xlabel('Sample index')\n",
    "    ax.set_ylabel('Percent calls')\n",
    "    ax.set_title(title)\n",
    "    handles = [mpl.patches.Patch(color=palette[0]),\n",
    "               mpl.patches.Patch(color=palette[1])]\n",
    "    ax.legend(handles=handles, labels=['BFM', 'AOM'], title='Population',\n",
    "              bbox_to_anchor=(1, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at missingness first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_genotype_frequency(pc_missing, 'Missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one sample with higher missingness than the others. Which one is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argsort(pc_missing)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig a little more into this sample. Is the excess missingness spread over the whole genome, or only in a specific region? Choose two other samples to compare with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g_strange = genotypes_subset.take([60, 100, 127], axis=1)\n",
    "g_strange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locate missing calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_missing = g_strange.is_missing()[:]\n",
    "is_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot missingness for each sample over the chromosome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = variants_pass['POS'][:]\n",
    "window_size = 100000\n",
    "y1, windows, _ = allel.stats.windowed_statistic(pos, is_missing[:, 0], statistic=np.count_nonzero, size=window_size)\n",
    "y2, windows, _ = allel.stats.windowed_statistic(pos, is_missing[:, 1], statistic=np.count_nonzero, size=window_size)\n",
    "y3, windows, _ = allel.stats.windowed_statistic(pos, is_missing[:, 2], statistic=np.count_nonzero, size=window_size)\n",
    "x = windows.mean(axis=1)\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "sns.despine(ax=ax, offset=10)\n",
    "ax.plot(x, y1 * 100 / window_size, lw=1)\n",
    "ax.plot(x, y2 * 100 / window_size, lw=1)\n",
    "ax.plot(x, y3 * 100 / window_size, lw=1)\n",
    "ax.set_xlabel('Position (bp)')\n",
    "ax.set_ylabel('Percent calls');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample with higher missingness (in red) has generally higher missingness over the whole chromosome. \n",
    "\n",
    "Let's look at heterozygosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_genotype_frequency(pc_het, 'Heterozygous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No samples stand out, although it looks like there is a general trend for lower heterozogysity in the AOM population.\n",
    "\n",
    "The level of missingness in sample 127 is higher than other samples but not that high in absolute terms, so let's leave it in for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allele count\n",
    "\n",
    "As a first step into doing some population genetic analyses, let's perform an allele count within each of the two populations we've selected. This just means, for each SNP, counting how many copies of the reference allele (0) and each of the alternate alleles (1, 2, 3) are observed.\n",
    "\n",
    "To set this up, define a dictionary mapping population names onto the indices of samples within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subpops = {\n",
    "    'all': list(range(len(samples_subset))),\n",
    "    'BFM': samples_subset[samples_subset.population == 'BFM'].index.tolist(),\n",
    "    'AOM': samples_subset[samples_subset.population == 'AOM'].index.tolist(),\n",
    "}\n",
    "subpops['AOM'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the allele count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ac_subpops = genotypes_subset.count_alleles_subpops(subpops, max_allele=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ac_subpops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in the table above has allele counts for a population, where \"all\" means the union of both populations. We can pull out a single column, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ac_subpops['AOM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the AOM population, at the fifth variant (index 4) we observe 118 copies of the reference allele (0) and 2 copies of the first alternate allele (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate segregating variants\n",
    "\n",
    "There are lots of SNPs which do not segregate in either of these populations are so are not interesting for any analysis of these populations. We might as well get rid of them.\n",
    "\n",
    "How many segregating SNPs are there in each population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pop in 'all', 'BFM', 'AOM':\n",
    "    print(pop, ac_subpops[pop].count_segregating())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locate SNPs that are segregating in the union of our two selected populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_seg = ac_subpops['all'].is_segregating()[:]\n",
    "is_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset genotypes again to keep only the segregating SNPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genotypes_seg = genotypes_subset.compress(is_seg, axis=0)\n",
    "genotypes_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the variants and allele counts too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variants_seg = variants_pass.compress(is_seg)\n",
    "variants_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ac_seg = ac_subpops.compress(is_seg)\n",
    "ac_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population differentiation\n",
    "\n",
    "Are these two populations genetically different? To get a first impression, let's plot the alternate allele counts from each population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jsfs = allel.stats.joint_sfs(ac_seg['BFM'][:, 1], ac_seg['AOM'][:, 1])\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "allel.stats.plot_joint_sfs(jsfs, ax=ax)\n",
    "ax.set_ylabel('Alternate allele count, BFM')\n",
    "ax.set_xlabel('Alternate allele count, AOM');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the alternate allele counts are correlated, meaning there is some relationship between these two populations, however there are plenty of SNPs off the diagonal, suggesting there is also some differentiation.\n",
    "\n",
    "Let's compute average Fst, a statistic which summarises the difference in allele frequencies averaged over all SNPs. This also includes an estimate of standard error via jacknifing in blocks of 100,000 SNPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fst, fst_se, _, _ = allel.stats.blockwise_hudson_fst(ac_seg['BFM'], ac_seg['AOM'], blen=100000)\n",
    "print(\"Hudson's Fst: %.3f +/- %.3f\" % (fst, fst_se))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to plot Fst in windows over the chromosome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_fst(ac1, ac2, pos, blen=2000):\n",
    "    \n",
    "    fst, se, vb, _ = allel.stats.blockwise_hudson_fst(ac1, ac2, blen=blen)\n",
    "    \n",
    "    # use the per-block average Fst as the Y coordinate\n",
    "    y = vb\n",
    "    \n",
    "    # use the block centres as the X coordinate\n",
    "    x = allel.stats.moving_statistic(pos, statistic=lambda v: (v[0] + v[-1]) / 2, size=blen)\n",
    "    \n",
    "    # plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    sns.despine(ax=ax, offset=10)\n",
    "    ax.plot(x, y, 'k-', lw=.5)\n",
    "    ax.set_ylabel('$F_{ST}$')\n",
    "    ax.set_xlabel('Chromosome %s position (bp)' % chrom)\n",
    "    ax.set_xlim(0, pos.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are any chromosome regions particularly differentiated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_fst(ac_seg['BFM'], ac_seg['AOM'], variants_seg['POS'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there could be an interesting signal of differentiation around 11Mbp. In fact I happen to know from other analyses that signal is driven by evolution in genes involved in the mosquito's immune response to parasites, first described in [White et al. (2011)](http://www.ncbi.nlm.nih.gov/pubmed/21173248).\n",
    "\n",
    "There are a number of subtleties to Fst analysis which I haven't mentioned here, but you can read more about [estimating Fst](http://alimanfoo.github.io/2015/09/21/estimating-fst.html) on my blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site frequency spectra\n",
    "\n",
    "While we're looking at allele counts, let's also plot a site frequency spectrum for each population, which gives another summary of the data and is also informative about demographic history.\n",
    "\n",
    "To do this we really do need to restrict to biallelic SNPs, so let's do that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_biallelic_01 = (ac_seg['all'].allelism()[:] == 2) & (ac_seg['all'].max_allele()[:] == 1)\n",
    "ac1 = ac_seg['BFM'].compress(is_biallelic_01, axis=0)[:, :2]\n",
    "ac2 = ac_seg['AOM'].compress(is_biallelic_01, axis=0)[:, :2]\n",
    "ac1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now plot folded site frequency spectra, scaled such that populations with constant size should have a spectrum close to horizontal (constant across allele frequencies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.despine(ax=ax, offset=10)\n",
    "sfs1 = allel.stats.sfs_folded_scaled(ac1)\n",
    "allel.stats.plot_sfs_folded_scaled(sfs1, ax=ax, label='BFM', n=ac1.sum(axis=1).max())\n",
    "sfs2 = allel.stats.sfs_folded_scaled(ac2)\n",
    "allel.stats.plot_sfs_folded_scaled(sfs2, ax=ax, label='AOM', n=ac2.sum(axis=1).max())\n",
    "ax.legend()\n",
    "ax.set_title('Scaled folded site frequency spectra')\n",
    "# workaround bug in scikit-allel re axis naming\n",
    "ax.set_xlabel('minor allele frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectra are very different for the two populations. BFM has an excess of rare variants, suggesting a population expansion, while AOM is closer to neutral expectation, suggesting a more stable population size.\n",
    "\n",
    "We can also plot Tajima's D, which is a summary of the site frequency spectrum, over the chromosome, to see if there are any interesting localised variations in this trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute windows with equal numbers of SNPs\n",
    "pos = variants_seg['POS'][:]\n",
    "windows = allel.stats.moving_statistic(pos, statistic=lambda v: [v[0], v[-1]], size=2000)\n",
    "x = np.asarray(windows).mean(axis=1)\n",
    "\n",
    "# compute Tajima's D\n",
    "y1, _, _ = allel.stats.windowed_tajima_d(pos, ac_seg['BFM'][:], windows=windows)\n",
    "y2, _, _ = allel.stats.windowed_tajima_d(pos, ac_seg['AOM'][:], windows=windows)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "sns.despine(ax=ax, offset=10)\n",
    "ax.plot(x, y1, lw=.5, label='BFM')\n",
    "ax.plot(x, y2, lw=.5, label='AOM')\n",
    "ax.set_ylabel(\"Tajima's $D$\")\n",
    "ax.set_xlabel('Chromosome %s position (bp)' % chrom)\n",
    "ax.set_xlim(0, pos.max())\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again there is an interesting signal around 11Mbp on BFM, suggesting some history of selection at that locus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Principal components analysis\n",
    "\n",
    "Finally, let's to a quick-and-dirty PCA to confirm our evidence for differentiation between these two populations and check if there is any other genetic structure within populations that we might have missed.\n",
    "\n",
    "First grab the allele counts for the union of the two populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ac = ac_seg['all'][:]\n",
    "ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the variants to use for the PCA, including only biallelic SNPs with a minor allele count above 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_selection = (ac.max_allele() == 1) & (ac[:, :2].min(axis=1) > 2)\n",
    "pca_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(pca_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now randomly downsample these SNPs purely for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = np.nonzero(pca_selection)[0]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices_ds = np.random.choice(indices, size=50000, replace=False)\n",
    "indices_ds.sort()\n",
    "indices_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the genotypes to keep only our selected SNPs for PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genotypes_pca = genotypes_seg.take(indices_ds, axis=0)\n",
    "genotypes_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the genotypes into an array of alternate allele counts per call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gn = genotypes_pca.to_n_alt()[:]\n",
    "gn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coords, model = allel.stats.pca(gn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coords.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pca_coords(coords, model, pc1, pc2, ax):\n",
    "    x = coords[:, pc1]\n",
    "    y = coords[:, pc2]\n",
    "    for pop in ['BFM', 'AOM']:\n",
    "        flt = (samples_subset.population == pop).values\n",
    "        ax.plot(x[flt], y[flt], marker='o', linestyle=' ', label=pop, markersize=6)\n",
    "    ax.set_xlabel('PC%s (%.1f%%)' % (pc1+1, model.explained_variance_ratio_[pc1]*100))\n",
    "    ax.set_ylabel('PC%s (%.1f%%)' % (pc2+1, model.explained_variance_ratio_[pc2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.despine(ax=ax, offset=10)\n",
    "plot_pca_coords(coords, model, 0, 1, ax)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.despine(ax=ax, offset=10)\n",
    "y = 100 * model.explained_variance_ratio_\n",
    "x = np.arange(len(y))\n",
    "ax.set_xticks(x + .4)\n",
    "ax.set_xticklabels(x + 1)\n",
    "ax.bar(x, y)\n",
    "ax.set_xlabel('Principal component')\n",
    "ax.set_ylabel('Variance explained (%)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this PCA we can see that only PC1 contains any information and this separates the two populations, so we have confirmed there is no other structure within these populations that needs to be taken into account.\n",
    "\n",
    "For running PCA with more populations there are a number of subtleties which I haven't covered here, for all the gory details see the [fast PCA](http://alimanfoo.github.io/2015/09/28/fast-pca.html) article on my blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Under the hood\n",
    "\n",
    "Here's a few notes on what's going on under the hood. If you want to know more, the best place to look is the [scikit-allel source code](https://github.com/cggh/scikit-allel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy arrays\n",
    "\n",
    "NumPy is the foundation for everything in scikit-allel. A NumPy array is an N-dimensional container for binary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([0, 4, 7])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# item access\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# slicing\n",
    "x[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy support array-oriented programming, which is both convenient and efficient, because looping is implemented internally in C code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = np.array([1, 6, 9])\n",
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-allel defines a number of conventions for storing variant call data using NumPy arrays. For example, a set of diploid genotype calls over *m* variants in *n* samples is stored as a NumPy array of integers with shape (m, n, 2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = allel.GenotypeArray([[[0, 0], [0, 1]],\n",
    "                         [[0, 1], [1, 1]],\n",
    "                         [[0, 2], [-1, -1]]], dtype='i1')\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``allel.GenotypeArray`` class is a sub-class of ``np.ndarray``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isinstance(g, np.ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the usual properties and methods of an ndarray are inherited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# obtain calls for the second variant in all samples\n",
    "g[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# obtain calls for the second sample in all variants\n",
    "g[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# obtain the genotype call for the second variant, second sample\n",
    "g[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a subset with only the first and third variants\n",
    "g.take([0, 2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find missing calls\n",
    "np.any(g < 0, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances of ``allel.GenotypeArray`` also have some extra properties and methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g.n_variants, g.n_samples, g.ploidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g.count_alleles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunked, compressed arrays\n",
    "\n",
    "The ``scikit-allel`` genotype array convention is flexible, allowing for multiallelic and polyploid genotype calls. However, it is not very compact, requiring 2 bytes of memory for each call. A set of calls for 10,000,000 SNPs in 1,000 samples thus requires 20G of memory.\n",
    "\n",
    "One option to work with large arrays is to use bit-packing, i.e., to pack two or more items of data into a single byte. E.g., this is what the [plink BED format](http://pngu.mgh.harvard.edu/~purcell/plink/binary.shtml) does. If you have have diploid calls that are only ever biallelic, then it is possible to fit 4 genotype calls into a single byte. This is 8 times smaller than the NumPy unpacked representation.\n",
    "\n",
    "However, coding against bit-packed data is not very convenient. Also, there are several libraries available for Python which allow N-dimensional arrays to be stored using **compression**: [h5py](http://www.h5py.org/), [bcolz](http://bcolz.blosc.org/en/latest/) and [zarr](http://zarr.readthedocs.io). Genotype data is usually extremely compressible due to sparsity - most calls are homozygous ref, i.e., (0, 0), so there are a lot of zeros. \n",
    "\n",
    "For example, the ``genotypes`` data we used above has calls for 16 million variants in 765 samples, yet requires only 1.2G of storage. In other words, there are more than 9 genotype calls per byte, which means that each genotype call requires less than a single bit on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this array are stored in an HDF5 file on disk and compressed using zlib, and achieve a compression ratio of 19.1 over an equivalent uncompressed NumPy array.\n",
    "\n",
    "To avoid having to decompress the entire dataset every time you want to access any part of it, the data are divided into chunks and each chunk is compressed. You have to choose the chunk shape, and there are some trade-offs regarding both the shape and size of a chunk. \n",
    "\n",
    "Here is the chunk shape for the ``genotypes`` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genotypes.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the dataset is broken into chunks where each chunk has data for 6553 variants and 10 samples.\n",
    "\n",
    "This gives a chunk size of ~128K (6553 \\* 10 \\* 2) which we have since found is not optimal - better performance is usually achieved with chunks that are at least 1M. However, performance is not bad and the data are publicly released so I haven't bothered to rechunk them.\n",
    "\n",
    "Chunked, compressed arrays can be stored either on disk (as for the ``genotypes`` dataset) or in main memory. E.g., in the tour above, I stored all the intermediate genotype arrays in memory, such as the ``genotypes_subset`` array, which can speed things up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genotypes_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform some operation over a chunked arrays, the best way is to compute the result for each chunk separately then combine the results for each chunk if needed. All functions in ``scikit-allel`` try to use a chunked implementation wherever possible, to avoid having to load large data uncompressed into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [alimanfoo.github.io](http://alimanfoo.github.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
